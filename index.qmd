---
title: "Project Title"
subtitle: "INFO 523 - Project Final"
author: 
  - name: "Team name - Team member 1, Team member 2,..."
    affiliations:
      - name: "School of Information, University of Arizona"
description: "Project description"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract
Add project abstract here.


## Introduction
Add project introduction here.

## Setup
```{python}
import pandas as pd
import glob
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier 
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import cross_validate
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
```



## EDA
```{python}


info = pd.read_csv('data/ODI_Match_info.csv')
info = info.rename(columns = {'id':'match_id'})

#append all files together
csv_files = ['data/output_1.csv','data/output_2.csv','data/output_3.csv','data/output_4.csv','data/output_5.csv','data/output_6.csv','data/output_7.csv','data/output_8.csv','data/output_9.csv']

matchData = pd.concat([pd.read_csv(f,low_memory=False) for f in csv_files ], ignore_index=True)

#merge frames on match ID column

totalData = pd.merge(matchData, info, on = 'match_id') #merge by identical column 'match_id'
totalData.drop(totalData.filter(regex='_y$').columns, axis=1, inplace=True) #drop duplicate columns

totalData = totalData.rename(columns = {'season_x':'season', 'venue_x':'venue'})

from02to22 = totalData[~totalData['season'].astype(str).str.startswith(('2023/2024','2023', '2022/23'))] #exclude 2023 data

from02to22
print(type(from02to22)) #confirm data is read in as a df
print(from02to22.shape) #confirm data shape
print(from02to22.dtypes) #understand the types of data in the df
print(from02to22.isna().sum()) #count NA values in columns
print(pd.DataFrame.describe(from02to22)) #descriptive function to look at dataframe)


winners = sns.countplot(data = from02to22, y = 'winner', order=from02to22['winner'].value_counts().index)
winners

corr = sns.pairplot(from02to22)
corr




```

## Data Cleaning
```{python}
#drop columns that have more than 1Million NaNs

colNaCounts = from02to22.isna().sum()


columns_to_drop = colNaCounts[colNaCounts >= 1000000].index.tolist()

# Drop identified columns from the DataFrame
from02to22 = from02to22.drop(columns=columns_to_drop)






#convert Nan cities to 'Unknown'
#drop winner NA columns
#convert NA player of match to 'unknown'
#convert NA umpire 3 to 'unknown'

from02to22['city'] = from02to22['city'].fillna('Unknown') 
from02to22['player_of_match'] = from02to22['player_of_match'].fillna('Unknown') 
from02to22['umpire3'] = from02to22['umpire3'].fillna('Unknown') 
from02to22 = from02to22.dropna(subset=['winner'])
from02to22 = from02to22.drop(columns = ['id'])


```

## Feature Engineering
```{python}
#normalize continuous data because we dont have a normal distribution

to_norm = (from02to22.select_dtypes(include =['int64', 'float64'])) #select continuous variables

continuous = MinMaxScaler().fit_transform(to_norm) #fit and transform min max scaler (normalizes)
continuous = pd.DataFrame(continuous, index = from02to22.index, columns = list(to_norm))

#encode categorical variables

to_encode = (from02to22.select_dtypes(include =['object'])) #test if anything is an object or category variable

label_encoders = [] #make new encoder for each column
encoded_data = pd.DataFrame()

# Iterate over each column in 'to_encode' and encode using a separate LabelEncoder
for col in to_encode:
    # Create a new instance of LabelEncoder for the current column
    encoder = LabelEncoder()
    
    # Fit and transform the data in 'from02to22[col]'
    encoded_data[col] = encoder.fit_transform(from02to22[col])
    
    # Store the encoder in the list
    label_encoders.append(encoder)

#patch the columns back together

data = pd.concat([encoded_data.reset_index(drop=True), continuous.reset_index(drop=True)], axis = 1) #reset indices to avoid errors in concat


```

## Feature Selection
```{python}
X = data.drop('winner', axis = 1) #input
y = data['winner'] #target

selector = SelectKBest(score_func= f_classif, k = 10)
top10 = selector.fit_transform(X,y) #create variable that is the top 10 best feautures
cols_idxs = selector.get_support(indices=True) #grab indices from feature cols, get_support is from **sklearn**
top10 = X.iloc[:,cols_idxs] #add columns from whole dataset to the selected columns dataset https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le
print(top10)

```

## Model Training (Decision Tree)
```{python}

#split data into training and testing
X_train, X_test, y_train, y_test = tts(top10, y, test_size = 0.2) #break into 4 groups for testing and training, make the training dataset 70% of the data and the testing dataset 30% https://builtin.com/data-science/train-test-split
#PCA
pca = PCA(n_components = 5) #go from 10 components to 5 components
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

X_train = X_train_pca
X_test = X_test_pca

#training and testing models

 #decision tree
dtc = DecisionTreeClassifier(criterion='gini')
dtc.fit(X_train, 
          y_train)
predictDTC = dtc.predict(X_test)
outcomeDTC = pd.DataFrame ({'Actual': y_test, 'Predicted': predictDTC})
outcomeDTC['Actual'] = label_encoders[8].inverse_transform(outcomeDTC['Actual'])
outcomeDTC['Predicted'] = label_encoders[8].inverse_transform(outcomeDTC['Predicted'])

key = (label_encoders[8].classes_)


```

## Model Validation
```{python}
#Evaluating models
#decision tree
cvDT = cross_validate(dtc,X_train, y_train)
print("cross validation of decision tree", cvDT)
print("accuracy of decision tree:", metrics.accuracy_score(y_test, predictDTC)) #testing how accuracy of the models
print("precision of decision tree:", metrics.precision_score(y_test, predictDTC, average = 'weighted'))
print("recall of decision tree:", metrics.recall_score(y_test, predictDTC, average = 'weighted'))
print("f1 of decision tree:", metrics.f1_score(y_test, predictDTC, average = 'weighted'))
metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(outcomeDTC['Actual'], outcomeDTC['Predicted'])).plot()


# y_pred_proba = dtc.predict_proba(X_test)[::,1] #need probabilities for roc curve
# fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
# plt.plot(fpr,tpr)
# plt.ylabel('True Positive Rate')
# plt.xlabel('False Positive Rate')
# plt.title('ROC Curve Decision Tree')
# plt.show()

# print('')




```

## Alternative Models
```{python}


```

### Decision Tree Classifier
```{python}

```

### Random Forest Classifier
```{python}

#decision tree
rfc = RandomForestClassifier(criterion='gini')
rfc.fit(X_train, 
          y_train)
predictRFC = rfc.predict(X_test)
outcomeRFC = pd.DataFrame ({'Actual': y_test, 'Predicted': predictRFC})
outcomeRFC['Actual'] = label_encoders[8].inverse_transform(outcomeRFC['Actual'])
outcomeRFC['Predicted'] = label_encoders[8].inverse_transform(outcomeRFC['Predicted'])

#Evaluating models
#random forest
cvRF = cross_validate(dtc,X_train, y_train)
print("cross validation of random forest", cvRF)
print("accuracy of random forest:", metrics.accuracy_score(y_test, predictRFC)) #testing how accuracy of the models
print("precision of random forest:", metrics.precision_score(y_test, predictRFC, average = 'weighted'))
print("recall of random forest:", metrics.recall_score(y_test, predictRFC, average = 'weighted'))
print("f1 of random forest:", metrics.f1_score(y_test, predictRFC, average = 'weighted'))
metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(outcomeRFC['Actual'], outcomeRFC['Predicted'])).plot()
```

## Model Comparison and Selection
```{python}

```

## Model Implementation with Dashboard
```{python}
#This data will be substituted with mocked "live" data when the code is integrated into the dashboard

```
